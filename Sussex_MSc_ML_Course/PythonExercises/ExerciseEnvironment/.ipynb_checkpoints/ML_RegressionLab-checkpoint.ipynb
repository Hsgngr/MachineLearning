{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tnPIhGezbB0l"
   },
   "source": [
    "# Introduction\n",
    "Welcome to todays lab session where we will focus on linear regression. This lab will all be in Python, but the majority of the code is provided for you. As well as this notebook, you could also take a look at the regression quiz on canvas.\n",
    "\n",
    "The easiest way to complete this session is to log into a Google account and save a copy to your google drive. Alternatively, you could save this document to your machine and open as a Jupyter notebook. This lab only requires 2 external libraries, matplotlib and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExAVAwKZbxO8",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Import numpy for linear algebra support\n",
    "import numpy as np\n",
    "# Matplotlib is our main data plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a training data range that's smaller than our test range, so we can demonstrate extrapolation\n",
    "x_train_range = [-20, 20]\n",
    "x_test_range = [-30, 30]\n",
    "x_test = np.linspace(*x_test_range, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WCi1o780fUCY"
   },
   "source": [
    "# Creating some test data\n",
    "We can generate our own data that fulfills our model assumptions to play with.\n",
    "\n",
    "Below is a function called create_data, which randomly generates a pair of linear model weights to synthesise some data. Note that the function takes a parameter called noise_sigma. This refers to the Gaussian distributed random noise that we expect to see in our model. It also takes n_samples, the number of data samples to generate.\n",
    "\n",
    "1. Try running the code below a few times, and note how the resulting plot changes each time you run it. Why is the plot changing? What do you notice about the plot for different model weight values?\n",
    "\n",
    "2. Try adjusting the value of sigma, what effect does this have on the plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oXqCjKfbfEvh"
   },
   "outputs": [],
   "source": [
    "# Let's start by defining a function some data that follows a linear assumption\n",
    "def create_data(noise_sigma=1.0, n_samples=10):\n",
    "  # Sample some data at even intervals in x. Choose n_sample points\n",
    "  x = np.linspace(*x_train_range, n_samples)\n",
    "  # Let's randomly generate some coefficient weights\n",
    "  intercept = np.random.randn()\n",
    "  linear_coeff = np.random.randn()\n",
    "  \n",
    "  # Regression defines a generative model of our observed target variables\n",
    "  # So we can use the same model to generate our training data.\n",
    "  y = x*linear_coeff + intercept\n",
    "  # We need to remember to add our residual model noise. We sample random normally\n",
    "  # distributed values and scale them by the standard deviation\n",
    "  y += np.random.randn(*y.shape) * noise_sigma\n",
    "\n",
    "  # Let's store the noise free, ground truth function in the right range\n",
    "  y_gt = x_test*linear_coeff + intercept\n",
    "\n",
    "  return x, y, (intercept, linear_coeff), y_gt\n",
    "\n",
    "# If we call this function it returns some x and y points as well as the real model weights\n",
    "x, y, weights, y_gt = create_data()\n",
    "# Show the ground truth function in cyan\n",
    "plt.plot(x_test, y_gt, 'c-')\n",
    "plt.scatter(x, y)\n",
    "plt.show()\n",
    "\n",
    "print(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FzaH3s4ihXxt"
   },
   "source": [
    "# Fitting a model to data\n",
    "As we've seen in the lectures, one way of fitting a linear regression model (estimating the parameters) is to calculate the psuedo-inverse of X. Where the optimal parameters w* are given by: $$\\mathbf{w}^* = (X^TX)^{-1}X^T\\mathbf{y} = X^{+}\\mathbf{y}$$, and $X^+$ is the psuedo-inverse (np.pinv) of X. For simple regression problems like we have here, this is an effective and simple solution.\n",
    "\n",
    "1. Read through the code below and see how this equates with the mathematics in the slides.\n",
    "\n",
    "2. Run the code for different generated data samples (running create_data again). How can we measure how well the model is doing? Try running this code for larger/smaller values of sigma. How does this affect the result?\n",
    "\n",
    "3. Try also adjusting the number of data samples. How does this affect the results?\n",
    "\n",
    "4. What was the cost function that is being minimised by this model? Can you calculate the value of cost function for the solution below? hint: [np.square](https://numpy.org/doc/1.18/reference/generated/numpy.square.html) and [np.mean](https://numpy.org/doc/1.18/reference/generated/numpy.mean.html) might be helpful\n",
    "\n",
    "5. How can we estimate $\\sigma$ from the fitted model? hint: think what sigma represents and look at the function [np.std](https://numpy.org/doc/1.18/reference/generated/numpy.std.html) What do you notice about this with respect to the cost function? and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QtAmpqvhiZf"
   },
   "outputs": [],
   "source": [
    "# augment our data to allow for a constant intercept\n",
    "def augment_x(data):\n",
    "  # We need to add a row ones to x to describe the constant intecept\n",
    "  augmented_data = np.stack([np.ones_like(data), data])\n",
    "  return augmented_data\n",
    "\n",
    "\n",
    "def fit_linear_regression(_x, _y):\n",
    "  _x = augment_x(_x)\n",
    "  # Estimate the weights by multipling the psuedo-inverse by y\n",
    "  estimated_weights = np.matmul(np.linalg.pinv(_x).transpose(), _y)\n",
    "  return estimated_weights\n",
    "\n",
    "# Produce our prediction for a given value of x for the fitted model weights\n",
    "def f_hat(_x, _w):\n",
    "  # Calculate our predicted points for each value of x\n",
    "  y_hat = np.matmul(augment_x(_x).T, _w)\n",
    "  return y_hat\n",
    "\n",
    "estimated_weights = fit_linear_regression(x, y)\n",
    "\n",
    "# Print the esimated weights and then the real weights\n",
    "print(estimated_weights, weights)\n",
    "\n",
    "# Calculate our predicted points for each value of x\n",
    "y_hat = f_hat(x, estimated_weights)\n",
    "\n",
    "# Plot the fitted function outside of the range with a black dotted line\n",
    "plt.plot(x_test, f_hat(x_test, estimated_weights), 'k--')\n",
    "# Plot the fitted function inside the training range with a blue line\n",
    "plt.plot(x, y_hat, 'b-')\n",
    "# Show the training points as red crosses\n",
    "plt.plot(x, y, 'rx')\n",
    "# Show the ground truth function in cyan\n",
    "plt.plot(x_test, y_gt, 'c-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yoK2g1uam-1e"
   },
   "source": [
    "# Increasing the complexity of the generated data\n",
    "As we discussed in the lecture, we make the linear regression model more complex by augmenting the data. \n",
    "1. Take a look at the new function below, there's one small change you need to make for it to generate data according to a quadratic, e.g. $y = w_0 + w_1x +w_2x^2$. Can you spot it? \n",
    "2. Run the simple model fitting from before, how does it do on data that follows the more complex function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UcpsUwx4nxBC"
   },
   "outputs": [],
   "source": [
    "def create_more_complex_data(noise_sigma=1.0, n_samples=10):\n",
    "  # Sample some data at even intervals in x\n",
    "  x = np.linspace(*x_train_range, n_samples)\n",
    "  # Let's randomly generate some coefficient weights\n",
    "  intercept = np.random.randn()\n",
    "  linear_coeff = np.random.randn()\n",
    "  quad_coeff = np.random.randn()*0.0\n",
    "  \n",
    "  # Regression defines a generative model of our observed target variables\n",
    "  # So we can use the same model to generate our training data.\n",
    "  y = np.square(x)*quad_coeff + x*linear_coeff + intercept\n",
    "  # We need to remember to add our residual model noise. We sample random normally\n",
    "  # distributed values and scale them by the standard deviation\n",
    "  y += np.random.randn(*y.shape) * noise_sigma\n",
    "\n",
    "  # Let's store the noise free, ground truth function in the right range\n",
    "  y_gt = np.square(x_test)*quad_coeff + x_test*linear_coeff + intercept\n",
    "\n",
    "  return x, y, (intercept, linear_coeff, quad_coeff), y_gt\n",
    "\n",
    "x, y, weights, y_gt = create_more_complex_data(4.0, 15)\n",
    "plt.scatter(x, y)\n",
    "# Show the ground truth function in cyan\n",
    "plt.plot(x_test, y_gt, 'c-')\n",
    "plt.show()\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ouO4eeeYqkDt"
   },
   "outputs": [],
   "source": [
    "estimated_weights = fit_linear_regression(x, y)\n",
    "\n",
    "# Print the esimated weights and then the real weights\n",
    "print(estimated_weights, weights)\n",
    "\n",
    "# Calculate our predicted points for each value of x\n",
    "y_hat = f_hat(x, estimated_weights)\n",
    "\n",
    "# Plot the fitted function outside of the range with a black dotted line\n",
    "plt.plot(x_test, f_hat(x_test, estimated_weights), 'k--')\n",
    "# Show the ground truth function in cyan\n",
    "plt.plot(x_test, y_gt, 'c-')\n",
    "plt.plot(x, y_hat, 'b-')\n",
    "plt.plot(x, y, 'rx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IcVPg7XMsU--"
   },
   "source": [
    "# Increasing the complexity of the model\n",
    "Now let's make the model accordingly more complex\n",
    "\n",
    "1. Modify the function transform_data, which is nested in augment_x_v2 to make the model fit a quadratic to the data. The default return of np.zeros_like() doesn't have any effect on the model fitting, why?\n",
    "\n",
    "2. How can we modify this code to fit a 3rd order polynomial, or an nth order polynomial. Hint: add more elements to the np.stack list and look at [np.power](https://numpy.org/doc/1.18/reference/generated/numpy.power.html).\n",
    "\n",
    "3. How does the more complex model react to noisier data/fewer samples? Is it more of less affected?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gFham06s0EP"
   },
   "outputs": [],
   "source": [
    "# augment our data to allow for a constant intercept and other functions \n",
    "# of the data to be used.\n",
    "def augment_x_v2(data):\n",
    "  def transform_data(data):\n",
    "    return np.zeros_like(data)\n",
    "  # We need to add a row ones to x to describe the constant intecept\n",
    "  augmented_data = np.stack([np.ones_like(data), data, transform_data(data)])\n",
    "  return augmented_data\n",
    "\n",
    "\n",
    "def fit_linear_regression_v2(_x, _y):\n",
    "  _x = augment_x_v2(_x)\n",
    "  estimated_weights = np.matmul(np.linalg.pinv(_x).transpose(), _y)\n",
    "  return estimated_weights\n",
    "\n",
    "def f_hat_v2(_x, _w):\n",
    "  # Calculate our predicted points for each value of x\n",
    "  y_hat = np.matmul(augment_x_v2(_x).T, _w)\n",
    "  return y_hat\n",
    "\n",
    "estimated_weights = fit_linear_regression_v2(x, y)\n",
    "\n",
    "\n",
    "# Print the esimated weights and then the real weights\n",
    "print(estimated_weights, weights)\n",
    "\n",
    "y_hat = f_hat_v2(x, estimated_weights)\n",
    "\n",
    "cost = np.mean(np.square(y_hat - y))\n",
    "std = np.std(y_hat-y)\n",
    "print(cost, std)\n",
    "# Plot the fitted function outside of the range with a black dotted line\n",
    "plt.plot(x_test, f_hat_v2(x_test, estimated_weights), 'k--')\n",
    "plt.plot(x, y_hat, 'b-')\n",
    "plt.plot(x, y, 'rx')\n",
    "plt.plot(x_test, y_gt, 'c-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JM_GFimSzxB_"
   },
   "source": [
    "# Calculating the error on some test points\n",
    "In the real world, we'll never know the true underlying function! Instead we'll have some test data that we've held out, which we hope was generated using the same process as the training data.\n",
    "\n",
    "Here, let's calculate the test data, using the same generating model but randomly sampled across the full range and with different noise realisations. \n",
    "1. Calculate the test error for the linear model trained on the more complex data.\n",
    "2. Calculate the test error for the more complex models trained on the complex data. Which model complexity works best? How is this model choice modified when sigma/n_samples increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTwFpyJo0Hss"
   },
   "outputs": [],
   "source": [
    "def create_test_data(weights, noise_sigma=1.0):\n",
    "  # Sample some data at random intervals in the test data ramge\n",
    "  x = np.random.random(30)*(x_test_range[1]-x_test_range[0]) + x_test_range[0]\n",
    "  \n",
    "  # Regression defines a generative model of our observed target variables\n",
    "  # So we can use the same model to generate our training data.\n",
    "  y = np.square(x)*weights[2] + x*weights[1] + weights[0]\n",
    "  # We need to remember to add our residual model noise. We sample random normally\n",
    "  # distributed values and scale them by the standard deviation\n",
    "  y += np.random.randn(*y.shape) * noise_sigma\n",
    "\n",
    "  return x, y\n",
    "\n",
    "x_test, y_test = create_test_data(weights, 1.0)\n",
    "\n",
    "# Plot the test points\n",
    "plt.plot(x_test, y_test, 'cx')\n",
    "# Plot the predicted location of the test points\n",
    "plt.plot(x_test, f_hat_v2(x_test, estimated_weights), 'ko')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWoX0SojyWGb"
   },
   "source": [
    "# Extensions\n",
    "These are all completely optional and there is no perscribed order.\n",
    "\n",
    "+ Modify the data generating function to be more complex, maybe use some trigonometric functions like cos/sin.\n",
    "\n",
    "+ Modify the augment_x_v2 function to include some different functions, such as cos/sin etc.\n",
    "\n",
    "+ The bias-variance decomposition lets us examine the effect on the learned models function when we refit the model using different samples of our dataset. Try generating several versions of the dataset with the same underlying generating function, but with different realisations of added noise. Fit a model using each of these versions of the dataset, and calculate the bias and variance of the model solutions. See this [tutorial](https://towardsdatascience.com/the-bias-variance-tradeoff-8818f41e39e9) for some ideas or check Chris Bishop's [book](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) on page 147.\n",
    "\n",
    "+ Try introducing some regularisation on the model weights. Where we introduce a squared regulariser by implementing $\\mathbf{w} = (X^TX + N\\lambda I)^-1X^Ty$. I is an identity matrix np.eye $\\lambda$ is a vector of regularisation parameters and N is the number of data samples. Plot how the train/test error varies with $\\lambda$. Maybe look at Novi's code [here](https://colab.research.google.com/drive/1Gt2WfpXARe3qlxVu44GvYQakG88SfsPX#scrollTo=mFu9ZO9xpQGR&forceEdit=true&offline=true&sandboxMode=true) for some inspiration."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_RegressionLab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
