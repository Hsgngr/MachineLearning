{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1) Try to build a classifier for MNIST dataset that achieves over 97% accuracy on the test set. Hint: the KNeighborsClassifier works quite well for this task.) You just need to find a good hyperparameter values (try a grid search on the weights and n_neighbors hyperparameters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (70000, 784)\n",
      "y shape: (70000,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()\n",
    "\n",
    "X,y = mnist['data'], mnist['target']\n",
    "print('X shape:',X.shape)\n",
    "print('y shape:',y.shape)\n",
    "\n",
    "#There are 70,000 images and each image has 784 features. This is because each image is\n",
    "#28 x 28 pixels and each feature simply represents one pixel's intensity from 0 to 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Train-Test Datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X,y, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'n_neighbors':[2,3,4,5,8], 'weights':('uniform', 'distance'), verbose =3}\n",
    "kn_clf = KNeighborsClassifier(n_jobs= -1)\n",
    "clf = GridSearchCV(kn_clf, parameters)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Write a function that can shift an MNIST image in any direction (left, right, up, or down) bt one pixel. Then for each image in the training set, create four shifted copies (one per direction) and add them to the training set. Finally, train your best model on this expanded training set and measure its accuracy on the test set. You should observe that your model performs even better now! This technique of artificially growing the training set is called \"data augmentation\" or \"training set expansion\".\n",
    "\n",
    "Note: You can use shift() function from the scipy.ndimage.interpolation module. For example shift(image,[2,1], cval=0) shifts the image two pixel down an one pixel to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Tackle the Titanic dataset. A great place to start is on Kaggle (https://www.kaggle.com/c/titanic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Build a spam classifier (a more challenging exercise):\n",
    "* Download examples of spam and ham from Apache Spam Assasins's datasets (https://homl.info/spamassasin)\n",
    "* Unzip the datasets and familiarize yourself with the data format\n",
    "* Split the datasets into a training set and a test set.\n",
    "* Write a data preparation pipeline to convert each email into a feature vector.\n",
    "Your preparation pipeline should transform an email into a (sparse) vector that indicates the presence or absence of each possible word. For example, if all emails only ever contain four words, \"Hello\", \"how\", \"are\", \"you\" then the email then the email \"Hello you Hello Hello you\" would be converted into vector [1,0,0,1] (meaning [\"Hello\" is present, \"how\" is absent, \"are\" is absent, \"you\" is present]) or [3,0,0,2] if you prefer to count the number of occurrences of each word.\n",
    "\n",
    "You may want to add hyperparameters to your preparation pipeline to control whether or not strip off email headers, convert each email to lowercase, remove punctuation, replace all URLs with \"URL\" replace all numbers with \"NUMBER\" or even perform stemming (i.e trim off word endings; there are Python libraries available to do this)\n",
    "\n",
    "Finally, try out several classifiers and see if you can build a great spam classifier, with both high recall and high precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
